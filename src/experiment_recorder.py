import mlflow
import os
import requests
from urllib.parse import urljoin
from logging_utils import log
import pandas as pd

class ExperimentRecorder:
    def __init__(self, tracking_uri=None):
        """Initialize the ExperimentRecorder with MLflow tracking URI.
        
        Args:
            tracking_uri (str, optional): MLflow tracking URI. If None, uses default localhost:5000.
        """
        if tracking_uri is None:
            tracking_uri = "http://localhost:5000"
        elif not tracking_uri.startswith(('http://', 'https://')):
            raise ValueError("tracking_uri must be an HTTP(S) URL")
        
        self.tracking_uri = tracking_uri

    def _is_mlflow_server_running(self):
        """Check if MLflow server is running and accessible."""
        try:
            response = requests.get(self.tracking_uri)
            return response.status_code == 200
        except requests.exceptions.ConnectionError:
            return False

    def get_experiments(self, exclude_test_experiment=False):
        """Get all MLflow experiments with their details.
        
        Args:
            exclude_test_experiment (bool, optional): If True, excludes experiments with 'test' in their name. Defaults to False.
            
        Returns:
            list: List of dictionaries containing experiment and run details
        """
        if not self._is_mlflow_server_running():
            raise RuntimeError("MLflow server not running. Please start it manually using 'mlflow ui' command.")

        try:
            # Set up MLflow tracking
            mlflow.set_tracking_uri(self.tracking_uri)
            
            # Get all experiments
            experiments = mlflow.search_experiments()
            run_details = []
            
            for exp in experiments:
                # Skip test experiments if exclude_test_experiment is True
                if exclude_test_experiment and 'test' in exp.name.lower():
                    continue
                    
                # Get all runs for this experiment
                runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])
                
                for _, run in runs.iterrows():
                    run_info = {
                        'experiment_name': exp.name,
                        'experiment_id': exp.experiment_id,
                        'run_id': run['run_id'],
                        'accuracy': run.get('metrics.accuracy'),
                        'description': run.get('tags.description')
                    }
                    run_details.append(run_info)
            
            return run_details
                
        except Exception as e:
            log.info(f"Error getting experiments: {str(e)}")
            raise

    def display_experiments(self, experiments=None):
        """Display experiment details in a formatted way.
        
        Args:
            experiments (list, optional): List of experiment details. If None, gets experiments using get_experiments()
            
        Returns:
            pd.DataFrame: DataFrame containing experiment details sorted by accuracy
        """
        if experiments is None:
            experiments = self.get_experiments()
            
        # Sort experiments by accuracy in descending order
        sorted_experiments = sorted(
            [exp for exp in experiments if exp['accuracy'] is not None],
            key=lambda x: x['accuracy'],
            reverse=True
        )
        
        # Convert to DataFrame
        experiment_summary_df = pd.DataFrame(sorted_experiments)
        
        # Drop experiment_id column
        experiment_summary_df = experiment_summary_df.drop('experiment_id', axis=1)
        
        # Rename columns for better readability
        experiment_summary_df = experiment_summary_df.rename(columns={
            'experiment_name': 'Experiment Name',
            'run_id': 'Run ID',
            'accuracy': 'Accuracy',
            'description': 'Description'
        })
        
        # Set description to "No description" where None
        experiment_summary_df['Description'] = experiment_summary_df['Description'].fillna('No description')
        
        # Format accuracy to 4 decimal places
        experiment_summary_df['Accuracy'] = experiment_summary_df['Accuracy'].round(4)
        
        log.info("\nMLflow Experiments Summary (sorted by accuracy):")
        log.info(experiment_summary_df.to_string(index=False))
        
        return experiment_summary_df

    def list_experiments(self):
        """List all MLflow experiments and their best accuracies.
        This is a convenience method that combines get_experiments and display_experiments.
        """
        experiments = self.get_experiments()
        self.display_experiments(experiments)

    def record_experiment(self, experiment_name, model, metrics, params, requirements_path, 
    training_data=None, preprocessing_functions=None, engineering_functions=None, description=None):
        """Record experiment details using MLflow.
        
        Args:
            experiment_name (str): Name of the experiment
            model: The trained model to log
            metrics (dict): Dictionary of metrics to log
            params (dict): Dictionary of parameters to log
            requirements_path (str): Path to requirements.txt
            training_data (pd.DataFrame, optional): Training data to log as artifact
            preprocessing_functions (list, optional): List of functions used in preprocessing
            engineering_functions (list, optional): List of functions used in feature engineering
            description (str, optional): Description of the experiment
        """
        if not self._is_mlflow_server_running():
            raise RuntimeError("MLflow server not running. Please start it manually using 'mlflow ui' command.")

        try:
            # Set up MLflow tracking
            mlflow.set_tracking_uri(self.tracking_uri)
            experiment = mlflow.set_experiment(experiment_name)
            
            # Start MLflow run
            with mlflow.start_run():
                # Log description if provided
                if description:
                    mlflow.set_tag("description", description)
                    log.info(f"\nExperiment Description: {description}")
                
                # Log parameters
                for key, value in params.items():
                    mlflow.log_param(key, value)
                
                # Log metrics
                for key, value in metrics.items():
                    mlflow.log_metric(key, value)
                
                # Log model
                try:
                    mlflow.h2o.log_model(
                        model, 
                        "model",
                        pip_requirements=requirements_path
                    )
                except Exception as e:
                    # If model logging fails, it might be a mock model for testing
                    # In this case, we'll just log the model type as a tag
                    mlflow.set_tag("model_type", type(model).__name__)
                    log.info(f"Note: Using mock model for testing - {str(e)}")
                
                # Log training data if provided
                if training_data is not None:
                    # Save DataFrame to a temporary CSV file
                    temp_csv_path = "training_data.csv"
                    training_data.to_csv(temp_csv_path, index=False)
                    mlflow.log_artifact(temp_csv_path, "data")
                    # Clean up temporary file
                    os.remove(temp_csv_path)
                    
                    # Log DataFrame headers as a tag
                    headers_tag = ",".join(training_data.columns)
                    mlflow.set_tag("dataframe_headers", headers_tag)
                    
                    # Log feature engineering functions as tags
                    if preprocessing_functions:
                        mlflow.set_tag("preprocessing_functions", ",".join(preprocessing_functions))
                        log.info("\nPreprocessing functions used:")
                        for func in preprocessing_functions:
                            log.info(f"  - {func}")
                    
                    if engineering_functions:
                        mlflow.set_tag("engineering_functions", ",".join(engineering_functions))
                        log.info("\nFeature engineering functions used:")
                        for func in engineering_functions:
                            log.info(f"  - {func}")
                
                # Get and print artifact URI
                model_uri = mlflow.get_artifact_uri("model")
                log.info(f'AutoML best model saved in {model_uri}')
                log.info(f'üèÉ View run at: {mlflow.get_tracking_uri()}/#/experiments/{experiment.experiment_id}/runs/{mlflow.active_run().info.run_id}')
                log.info(f'üß™ View experiment at: {mlflow.get_tracking_uri()}/#/experiments/{experiment.experiment_id}')
        except Exception as e:
            log.info(f"Error recording experiment: {str(e)}")
            raise 
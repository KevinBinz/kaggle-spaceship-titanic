{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c spaceship-titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRawDF = pd.read_csv('train.csv')\n",
    "testRawDF = pd.read_csv('test.csv')\n",
    "def curate(df):\n",
    "    df[['Deck', 'CabinNum', 'Side']] = df['Cabin'].str.split('/', expand=True, n=3)\n",
    "    df['CabinNumLen'] = df['CabinNum'].str.len()\n",
    "    df['CabinNum'] = pd.to_numeric(df['CabinNum'], errors='coerce')\n",
    "    df['CabinRegion'] = pd.qcut(df['CabinNum'], q=7)\n",
    "    df['AgeDecile'] = pd.qcut(df['Age'], q=10)\n",
    "\n",
    "    df[['FirstName', 'LastName']] = df['Name'].str.split(' ', expand=True, n=2)\n",
    "    df['GroupNum'] = df['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "\n",
    "    df['FamilySize'] = df.groupby(['LastName'])['LastName'].transform('size')\n",
    "    df['GroupSize'] = df.groupby(['GroupNum'])['GroupNum'].transform('size')\n",
    "    df['CabinSize'] = df.groupby(['CabinNum'])['CabinNum'].transform('size')\n",
    "\n",
    "    df['GroupSize'] = df.groupby(['GroupNum'])['GroupNum'].transform('size')\n",
    "    df['CabinSize'] = df.groupby(['CabinNum'])['CabinNum'].transform('size') \n",
    "\n",
    "    df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].fillna(0, inplace=True)\n",
    "    df['Expenditure'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n",
    "    df['LogExpenditure'] = np.log(df['Expenditure'] + 1)\n",
    "    df['ZeroExpense'] = df['Expenditure'] == 0\n",
    "    return df\n",
    "\n",
    "trainProcessedDF = curate(trainRawDF)\n",
    "testProcessedDF = curate(testRawDF)\n",
    "\n",
    "log.info(trainProcessedDF.shape)\n",
    "trainProcessedDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHF = h2o.H2OFrame(trainProcessedDF)\n",
    "testHF = h2o.H2OFrame(testProcessedDF)\n",
    "\n",
    "trainHF.describe()\n",
    "x = trainHF.columns\n",
    "y = \"Transported\"\n",
    "trainHF[y] = trainHF[y].asfactor()\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_models\": 10,  # Number of models to train\n",
    "    \"seed\": 42,  # Random seed\n",
    "    \"max_runtime_secs\": 7200,  # Time in seconds\n",
    "    \"sort_metric\": \"accuracy\"\n",
    "}\n",
    "\n",
    "aml = H2OAutoML(**params)\n",
    "aml.train(x=x, y=y, training_frame=trainHF)\n",
    "log.info(aml.leaderboard)\n",
    "model_path = h2o.save_model(model=aml.leader, path=\"/tmp/mymodel\", force=True)\n",
    "log.info(model_path)\n",
    "\n",
    "# Raw: Accuracy 79.1 - 80.1%\n",
    "# Curated: Accuracy 79.4 - 80.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id                                       accuracy       auc    logloss     aucpr    mean_per_class_error      rmse       mse\n",
      "GLM_1_AutoML_1_20250225_170628                 0.792822  0.882312   0.429498  0.892951                0.216531  0.37431   0.140108\n",
      "DRF_1_AutoML_1_20250225_170628                 0.799494  0.887478   0.431241  0.899816                0.212823  0.368555  0.135833\n",
      "DeepLearning_1_AutoML_1_20250225_170628        0.800299  0.887558   0.426185  0.899028                0.209818  0.371594  0.138082\n",
      "XRT_1_AutoML_1_20250225_170628                 0.80237   0.885706   0.426571  0.898413                0.20144   0.369724  0.136696\n",
      "GBM_5_AutoML_1_20250225_170628                 0.803405  0.897829   0.396356  0.910413                0.19707   0.359067  0.128929\n",
      "GBM_grid_1_AutoML_1_20250225_170628_model_1    0.80352   0.894891   0.402798  0.908003                0.200435  0.361557  0.130723\n",
      "GBM_1_AutoML_1_20250225_170628                 0.805131  0.894592   0.402693  0.908034                0.202142  0.361564  0.130729\n",
      "GBM_3_AutoML_1_20250225_170628                 0.805246  0.894321   0.404801  0.90709                 0.198321  0.362468  0.131383\n",
      "GBM_4_AutoML_1_20250225_170628                 0.805361  0.893361   0.411054  0.905943                0.198365  0.365769  0.133787\n",
      "GBM_2_AutoML_1_20250225_170628                 0.806856  0.895851   0.401209  0.908786                0.203422  0.360617  0.130044\n",
      "[12 rows x 8 columns]\n",
      "\n",
      "D:\\tmp\\mymodel\\GLM_1_AutoML_1_20250225_170628\n"
     ]
    }
   ],
   "source": [
    "log.info(aml.leaderboard)\n",
    "model_path = h2o.save_model(model=aml.leader, path=\"/tmp/mymodel\", force=True)\n",
    "log.info(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = h2o.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(model.varimp(use_pandas=True))\n",
    "model.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = curate(pd.read_csv('test.csv'))\n",
    "test = h2o.H2OFrame(test)\n",
    "test.describe()\n",
    "preds = aml.leader.predict(test)\n",
    "preds.describe()\n",
    "fullPredsHF = test.cbind(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullPredsDF = fullPredsHF.as_data_frame()\n",
    "fullPredsDF = fullPredsDF[['PassengerId', 'predict']]\n",
    "fullPredsDF = fullPredsDF.rename(columns={'predict': 'Transported'})\n",
    "fullPredsDF.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_id: 605466925374621565\n",
      "Artifact Location: mlflow-artifacts:/605466925374621565\n",
      "Lifecycle_stage: active\n",
      "Tracking uri: http://localhost:5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/27 09:38:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML best model saved in mlflow-artifacts:/605466925374621565/04aaabe067dd443b9ad7e909f6c4a88e/artifacts/model\n",
      "üèÉ View run aged-ape-271 at: http://localhost:5000/#/experiments/605466925374621565/runs/04aaabe067dd443b9ad7e909f6c4a88e\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/605466925374621565\n"
     ]
    }
   ],
   "source": [
    "# BEFORE RUNNING THIS CELL. In terminal, run \"mlflow UI\". Can check \"http://localhost:5000\" to inspect state.\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "experiment = mlflow.set_experiment(\"Titanic4\")\n",
    "log.info(f\"Experiment_id: {experiment.experiment_id}\")\n",
    "log.info(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "log.info(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")\n",
    "log.info(f\"Tracking uri: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "acc = model.accuracy()\n",
    "if isinstance(model.accuracy(), list):\n",
    "    acc = acc[0][1]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"max_models\", params[\"max_models\"])\n",
    "    mlflow.log_param(\"seed\", params[\"seed\"])\n",
    "    mlflow.log_param(\"max_runtime_secs\", params[\"max_runtime_secs\"])\n",
    "    mlflow.log_metric(\"logloss\", model.logloss())\n",
    "    mlflow.log_metric(\"auc\",model.auc())\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.h2o.log_model(model, \"model\", pip_requirements=\"../requirements.txt\")\n",
    "\n",
    "    model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "    log.info(f'AutoML best model saved in {model_uri}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c spaceship-titanic -f submission.csv -m \"First Pass with H2O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
